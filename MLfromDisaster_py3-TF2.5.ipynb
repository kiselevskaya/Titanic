{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7f51664",
   "metadata": {},
   "source": [
    "# Titanic - Machine Learning from Disaster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50563cff",
   "metadata": {},
   "source": [
    "##### Python 3.8 Tenserflow 2.7 enviroment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacad270",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7bc203",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955a98a4",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e697bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train.csv')\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80451cbe",
   "metadata": {},
   "source": [
    "## Copy Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d412a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782495f0",
   "metadata": {},
   "source": [
    "## Explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da6cfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f7126f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count missing values for train data\n",
    "train_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a518d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a heat map to see the correlation between the parameters and the target variable (Survived)\n",
    "# The higher the (absolut?) value the higher the correlation\n",
    "heatmap = sns.heatmap(train_df[['Survived', 'SibSp', 'Parch', 'Age', 'Fare', 'Pclass']].corr(), annot = True)\n",
    "sns.set(rc={'figure.figsize':(7,5)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ef3631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between Fare and Surviving\n",
    "plt.figure(figsize=(25, 7))\n",
    "plt.hist([train_df[train_df['Survived']==1]['Fare'], train_df[train_df['Survived']==0]['Fare']], \n",
    "         stacked=True, color=['dodgerblue','navy'],\n",
    "         bins=30, label=['Survived', 'Died']\n",
    "        )\n",
    "plt.xlabel('Fare')\n",
    "plt.ylabel('Number of passenger')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ecdbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between family size and Surviving\n",
    "plt.figure(figsize=(25, 7))\n",
    "plt.hist([train_df[train_df['Survived']==1]['Parch']+train_df[train_df['Survived']==1]['SibSp'],\n",
    "          train_df[train_df['Survived']==0]['Parch']+train_df[train_df['Survived']==0]['SibSp']], \n",
    "          stacked=True, color=['steelblue','navy'],\n",
    "          bins=30, label=['Survived', 'Died']\n",
    "         )\n",
    "plt.xlabel('Family')\n",
    "plt.ylabel('Number of passenger')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3508c30",
   "metadata": {},
   "source": [
    "###### Seems pasangers with 1 to 3 family members have more chances to survive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf6c78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between Age and Surviving\n",
    "plt.figure(figsize=(25, 7))\n",
    "plt.hist([train_df[train_df['Survived']==1]['Age'], train_df[train_df['Survived']==0]['Age']], \n",
    "         stacked=True, color=['darkturquoise','navy'],\n",
    "         bins=30, label=['Survived', 'Died']\n",
    "        )\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Number of passenger')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7990e3b2",
   "metadata": {},
   "source": [
    "###### Seems there is higher rate of surviving only among children around less than 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48568958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between Class and Surviving\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.hist([train_df[train_df['Survived']==1]['Pclass'], train_df[train_df['Survived']==0]['Pclass']], \n",
    "         stacked=True, color=['darkcyan','navy'],\n",
    "         bins=30, label=['Survived', 'Died']\n",
    "        )\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Number of passenger')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d06a303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between Gender and Surviving\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.hist([train_df[train_df['Survived']==1]['Sex'], train_df[train_df['Survived']==0]['Sex']], \n",
    "         stacked=True, color=['aqua','navy'],\n",
    "         bins=3, label=['Survived', 'Died']\n",
    "        )\n",
    "plt.xlabel('Sex')\n",
    "plt.ylabel('Number of Survived')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16e3540",
   "metadata": {},
   "source": [
    "###### Seems  women had more chances for surviving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c092b3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between Embarked and Surviving\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.hist([train_df[train_df['Survived']==1]['Embarked'].map(lambda x: str(x)), \n",
    "          train_df[train_df['Survived']==0]['Embarked'].map(lambda x: str(x))], \n",
    "          stacked=True, color=['teal','navy'],\n",
    "          bins=3, label=['Survived', 'Died']\n",
    "         )\n",
    "plt.xlabel('Embarked')\n",
    "plt.ylabel('Number of Survived')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c75c60",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2a0024",
   "metadata": {},
   "source": [
    "### Missing age values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9bc0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing age values with random between mean-std and mean+std\n",
    "age_mean = train_df[\"Age\"].mean()\n",
    "age_std = train_df[\"Age\"].std()\n",
    "\n",
    "# Remember age_mean and age_std to preprocess_data dictionary\n",
    "preprocess_data = {'age_mean':age_mean, 'age_std':age_std}\n",
    "\n",
    "def fill_missing_age(df, mean, std):\n",
    "    is_null = df[\"Age\"].isnull().sum()\n",
    "    # compute random numbers in range of mean +/- std and size of is_null values\n",
    "    rand_age = np.random.randint(mean - std, mean + std, size = is_null)\n",
    "    # fill NaN values in Age column with random values generated\n",
    "    age_slice = df[\"Age\"].copy()\n",
    "    age_slice[np.isnan(age_slice)] = rand_age\n",
    "    df[\"Age\"] = age_slice\n",
    "    df[\"Age\"] = df[\"Age\"].astype(int)\n",
    "    return df\n",
    "\n",
    "train_df = fill_missing_age(train_df, age_mean, age_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96a4187",
   "metadata": {},
   "source": [
    "### Missing fare values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42acbebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill fare missing values with mean \n",
    "fare_mean = train_df[\"Fare\"].mean()\n",
    "\n",
    "# Remember fare mean to preprocess_data dictionary\n",
    "preprocess_data['fare_mean'] = fare_mean\n",
    "    \n",
    "train_df['Fare'] = train_df['Fare'].fillna(train_df['Fare'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f370d58",
   "metadata": {},
   "source": [
    "### Missing embarked values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dbd1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values with most common\n",
    "embarked_mode = train_df['Embarked'].mode().iloc[0]\n",
    "\n",
    "# Remember embarked mode to preprocess_data dictionary\n",
    "preprocess_data['embarked_mode'] = embarked_mode\n",
    "\n",
    "train_df['Embarked'] = train_df['Embarked'].fillna(embarked_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47215947",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee75785",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1706a105",
   "metadata": {},
   "source": [
    "### Cabin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1252a9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn Cabin number into Deck \n",
    "decks = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'T', np.nan]\n",
    "\n",
    "# Remember decks to preprocess_data dictionary\n",
    "preprocess_data['decks'] = decks\n",
    "\n",
    "def substring_in(inp, sub):\n",
    "    for s in sub:\n",
    "        if str(inp) == str(s) or str(inp).find(s) != -1:\n",
    "            return s\n",
    "    return\n",
    "\n",
    "train_df['Deck'] = train_df['Cabin'].map(lambda x: substring_in(x, decks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23c61ac",
   "metadata": {},
   "source": [
    "### Relatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d5afee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine SibSp, Parch (siblings/spouse/parents/children) into new feature\n",
    "train_df['Relatives'] = train_df['SibSp']+train_df['Parch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2de1d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extend field 'Relatives' into 3 categories: Singleton, SmallFamily, LargerFamily\n",
    "def family_size(relatives):\n",
    "    if relatives == 0:\n",
    "        return 'Singleton'\n",
    "    elif 0 < relatives <= 3:\n",
    "        return 'SmallFamily'\n",
    "    else:\n",
    "        return 'LargerFamily'\n",
    "    \n",
    "train_df['Relatives'] = train_df['Relatives'].map(lambda x: family_size(x))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e80647",
   "metadata": {},
   "source": [
    "### Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15d1043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract from Name field title (it might be that some education or profession correlate with surviving)\n",
    "train_df['Title'] = train_df['Name'].map(lambda name: name.split(',')[1].split('.')[0].strip())\n",
    "train_df['Title'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50487e0",
   "metadata": {},
   "source": [
    "### Drop extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731170db",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop(['PassengerId', 'Name', 'SibSp', 'Parch', 'Ticket', 'Cabin'], axis=1)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab5ce67",
   "metadata": {},
   "source": [
    "## Scale numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afccfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88e85d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change Pclass type as it is a categorical variable\n",
    "train_df['Pclass'] = train_df['Pclass'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995d27dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Copy Dataframes before scaling\n",
    "train_scaled = train_df.copy()\n",
    "\n",
    "# Determine features to scale\n",
    "features_to_scale = list(train_scaled.select_dtypes(include=['float64', 'int32']).columns)\n",
    "\n",
    "# Fit scaler with train data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_scaled[features_to_scale])\n",
    "\n",
    "# Remember features_to_scale and scaler to scale_data dictionary\n",
    "scale_data = {'features_to_scale':features_to_scale, 'scaler':scaler}\n",
    "\n",
    "# Transform numerical values\n",
    "train_scaled[features_to_scale] = scaler.transform(train_scaled[features_to_scale])\n",
    "train_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0ee57e",
   "metadata": {},
   "source": [
    "## Transform categorical variables into dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2b1c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine other features to encode with dummies\n",
    "features_to_encode = list(train_scaled.select_dtypes(include=['object']).columns)\n",
    "features_to_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b22c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# Create binary dummies using OneHotEncoder to overcome the mismatch in features of train and test data\n",
    "encoder = OneHotEncoder(drop='first', dtype='int64', handle_unknown = 'ignore')\n",
    "\n",
    "# Remember features_to_encode, encoder to encode_data dictionary\n",
    "encode_data = {'features_to_encode':features_to_encode, 'encoder':encoder}\n",
    "\n",
    "encoded_df = encoder.fit_transform(train_scaled[features_to_encode])\n",
    "\n",
    "encoded_features = encoder.get_feature_names_out(features_to_encode)\n",
    "encoded_df = pd.DataFrame(encoded_df.todense(), columns=encoded_features)\n",
    "encoded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dee9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate scaled DataFrame and encoded DataFrame, drop already encoded features\n",
    "train_encoded = pd.concat([train_scaled.copy(), encoded_df], axis=1)\n",
    "train_encoded = train_encoded.drop(features_to_encode, axis=1)\n",
    "train_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67043613",
   "metadata": {},
   "source": [
    "## Save data used for preprocess, scale and encode features into files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c195c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_data, scale_data, encode_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41f87e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('preprocess_data.pkl', 'wb') as f:\n",
    "    pickle.dump(preprocess_data, f)\n",
    "    \n",
    "with open('scale_data.pkl', 'wb') as f:\n",
    "    pickle.dump(scale_data, f)\n",
    "    \n",
    "with open('encode_data.pkl', 'wb') as f:\n",
    "    pickle.dump(encode_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ce7da9",
   "metadata": {},
   "source": [
    "## Balance training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c8ea4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42febc58",
   "metadata": {},
   "source": [
    "## Divide training data into train, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42501c07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2eec025f",
   "metadata": {},
   "source": [
    "## Build prediction models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba490b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec184f76",
   "metadata": {},
   "source": [
    "## Load and  preprocess new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9a8d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load new data\n",
    "test_data = pd.read_csv('test.csv')\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e41daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy data and explore\n",
    "test_df = test_data.copy()\n",
    "test_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ddfd57",
   "metadata": {},
   "source": [
    "### Preprocess new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b686d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_data_preprocess(df):\n",
    "    # Load preprocessing data\n",
    "    with open('preprocess_data.pkl', 'rb') as f:\n",
    "        preprocess_data = pickle.load(f)\n",
    "        \n",
    "    # Fill missing age values\n",
    "    df = fill_missing_age(df, preprocess_data['age_mean'], preprocess_data['age_std'])\n",
    "    \n",
    "    # Fill missing fare values\n",
    "    df['Fare'] = df['Fare'].fillna(preprocess_data['fare_mean'])\n",
    "    \n",
    "    # Fill missing embarked values if any\n",
    "    df['Embarked'] = df['Embarked'].fillna(preprocess_data['embarked_mode'])\n",
    "    \n",
    "    # Turn Cabin number into Deck    \n",
    "    df['Deck'] = df['Cabin'].map(lambda x: substring_in(x, preprocess_data['decks']))\n",
    "    \n",
    "    # Combine SibSp, Parch (siblings/spouse/parents/children) into new feature\n",
    "    df['Relatives'] = df['SibSp']+df['Parch']\n",
    "    # Extend field 'Relatives' into 3 categories: Singleton, SmallFamily, LargerFamily with family_size() function\n",
    "    df['Relatives'] = df['Relatives'].map(lambda x: family_size(x))\n",
    "    \n",
    "    # Extract from Name field title (it might be that some education or profession correlate with surviving)\n",
    "    df['Title'] = df['Name'].map(lambda name: name.split(',')[1].split('.')[0].strip())\n",
    "    \n",
    "    # Drop extra features\n",
    "    df = df.drop(['PassengerId', 'Name', 'SibSp', 'Parch', 'Ticket', 'Cabin'], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "test_df = new_data_preprocess(test_df)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1166c8",
   "metadata": {},
   "source": [
    "### Scale new data numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2032b0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_data_scale(df):\n",
    "    # Change Pclass type as it is a categorical variable\n",
    "    df['Pclass'] = df['Pclass'].astype(str)\n",
    "\n",
    "    # Load scale data\n",
    "    with open('scale_data.pkl', 'rb') as f:\n",
    "        scale_data = pickle.load(f)\n",
    "\n",
    "    #Transform numerical values\n",
    "    scaler, features_to_scale = scale_data['scaler'], scale_data['features_to_scale']\n",
    "    df[features_to_scale] = scaler.transform(df[features_to_scale])\n",
    "        \n",
    "    return df\n",
    "\n",
    "# Copy Dataframes before scaling\n",
    "test_scaled = test_df.copy()\n",
    "\n",
    "test_scaled = new_data_scale(test_scaled)\n",
    "test_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708af17d",
   "metadata": {},
   "source": [
    "### Transform new data categorical variables into dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0566ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "def new_data_one_hot(df):\n",
    "    # Load encode data\n",
    "    with open('encode_data.pkl', 'rb') as f:\n",
    "        encode_data = pickle.load(f)\n",
    "    \n",
    "    # Transform features listed in features_to_encode into dummies\n",
    "    encoder, features_to_encode = encode_data['encoder'], encode_data['features_to_encode']\n",
    "    encoded_df = encoder.transform(df[features_to_encode])\n",
    "\n",
    "    # Get DataFrame from dummies matrix\n",
    "    encoded_features = encoder.get_feature_names_out(features_to_encode)    \n",
    "    encoded_df = pd.DataFrame(encoded_df.todense(), columns=encoded_features)\n",
    "    \n",
    "    # Concatenate scaled DataFrame and encoded DataFrame, drop already encoded features\n",
    "    df = pd.concat([df, encoded_df], axis=1)\n",
    "    df = df.drop(features_to_encode, axis=1)    \n",
    "    return df\n",
    "\n",
    "# Copy Dataframes before scaling\n",
    "test_encoded = test_scaled.copy()\n",
    "\n",
    "test_encoded = new_data_one_hot(test_encoded)\n",
    "test_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f32dd8",
   "metadata": {},
   "source": [
    "### Check there is no mismatch in features in train and new data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3722e258",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = train_encoded.columns.values[1:] == test_encoded.columns.values\n",
    "equal_features = comparison.all()\n",
    "  \n",
    "print(equal_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc8a228",
   "metadata": {},
   "source": [
    "## Run best perforemed model on new data and save prediction as submission.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfb5d1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.8_TF2.7",
   "language": "python",
   "name": "python3.8_tf2.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
